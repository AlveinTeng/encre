import {
  APIConnectionTimeoutError,
  APIUserAbortError,
  OpenAI as OpenAIClient,
} from 'openai';
import type { RequestOptions as OpenAIClientRequestOptions } from 'openai/core';

import { BaseLMCallOptions } from '../base';

export interface OpenAICallOptions extends BaseLMCallOptions {
  /**
   * Additional options to pass to the underlying axios request.
   */
  options: OpenAIClientRequestOptions;
}

export interface OpenAIBaseInput {
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
   * make the output more random, while lower values like 0.2 will make it more
   * focused and deterministic.
   *
   * We generally recommend altering this or `topP` but not both.
   */
  temperature: number;

  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the
   * model considers the results of the tokens with temperature probability mass. So 0.1
   * means only the tokens comprising the top 10% probability mass are considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   */
  topP: number;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
   * existing frequency in the text so far, decreasing the model's likelihood to
   * repeat the same line verbatim.
   *
   * {@see https://platform.openai.com/docs/guides/gpt/parameter-details}
   */
  frequencyPenalty: number;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on
   * whether they appear in the text so far, increasing the model's likelihood to
   * talk about new topics.
   *
   * {@see https://platform.openai.com/docs/guides/gpt/parameter-details}
   */
  presencePenalty: number;

  /**
   * How many completions to generate for each prompt.
   *
   * **Note:** Because this parameter generates many completions, it can quickly
   * consume your token quota. Use carefully and ensure that you have reasonable
   * settings for `max_tokens` and `stop`.
   */
  n: number;

  /**
   * Whether to stream back partial progress. If set, tokens will be sent as
   * data-only
   * {@see https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format}
   * as they become available, with the stream terminated by a `data: [DONE]`
   * message.
   * {@see https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb}.
   */
  stream: boolean;

  /**
   * ID of the model to use. You can use the
   * {@link https://platform.openai.com/docs/api-reference/models/list} API to
   * see all of your available models, or see
   * {@see https://platform.openai.com/docs/models/overview} for
   * descriptions of them.
   */
  modelName: string;

  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor
   * and detect abuse.
   * {@see https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids}.
   */
  user?: string;

  /**
   * The maximum number of [tokens](/tokenizer) to generate in the completion.
   *
   * The token count of your prompt plus `max_tokens` cannot exceed the model's
   * context length.
   * {@see https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb}
   * for counting tokens.
   */
  maxTokens?: number;

  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * Accepts a json object that maps tokens (specified by their token ID in the GPT
   * tokenizer) to an associated bias value from -100 to 100. You can use this
   * [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to
   * convert text to token IDs. Mathematically, the bias is added to the logits
   * generated by the model prior to sampling. The exact effect will vary per model,
   * but values between -1 and 1 should decrease or increase likelihood of selection;
   * values like -100 or 100 should result in a ban or exclusive selection of the
   * relevant token.
   *
   * As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token
   * from being generated.
   */
  logitBias?: Record<string, number>;

  /**
   * Include the log probabilities on the `logprobs` most likely tokens, as well the
   * chosen tokens. For example, if `logprobs` is 5, the API will return a list of
   * the 5 most likely tokens. The API will always return the `logprob` of the
   * sampled token, so there may be up to `logprobs+1` elements in the response.
   *
   * The maximum value for `logprobs` is 5.
   */
  logprobs?: number;

  /** Holds any additional parameters that are valid to pass to
   * {@link https://platform.openai.com/docs/api-reference/completions/create |
   * `openai.createCompletion`} that are not explicitly specified on this class.
   */
  additionalKwargs?: Record<string, unknown>;

  /**
   * Up to 4 sequences where the API will stop generating further tokens. The
   * returned text will not contain the stop sequence.
   */
  stopWords?: string[];

  /**
   * Timeout to use when making requests to OpenAI.
   */
  timeout?: number;

  /**
   * API key to use when making requests to OpenAI. Defaults to the value of
   * `OPENAI_API_KEY` environment variable.
   */
  openAIApiKey?: string;
}

export interface OpenAIInput extends OpenAIBaseInput {
  /**
   * Generates `bestOf` completions server-side and returns the "best" (the one with
   * the highest log probability per token). Results cannot be streamed.
   *
   * When used with `n`, `bestOf` controls the number of candidate completions and
   * `n` specifies how many to return â€“ `bestOf` must be greater than `n`.
   *
   * **Note:** Because this parameter generates many completions, it can quickly
   * consume your token quota. Use carefully and ensure that you have reasonable
   * settings for `maxToken` and `stopWords`.
   */
  bestOf?: number;
}

export interface OpenAIChatInput extends OpenAIBaseInput {
  /** ChatGPT messages to pass as a prefix to the prompt */
  chatMessages?: OpenAIClient.Chat.ChatCompletionMessageParam[];
}

export function checkModelForOpenAIChat(modelName?: string): boolean {
  return (
    modelName !== undefined &&
    (modelName.startsWith('gpt-3.5-turbo') || modelName.startsWith('gpt-4')) &&
    !modelName.includes('-instruct')
  );
}

export function wrapOpenAIClientError(e: Error): Error {
  let error: Error;
  if (e.constructor.name === APIConnectionTimeoutError.name) {
    error = new Error(e.message);
    error.name = 'TimeoutError';
  } else if (e.constructor.name === APIUserAbortError.name) {
    error = new Error(e.message);
    error.name = 'AbortError';
  } else {
    error = e;
  }

  return error;
}
